import streamlit as st
import os
from dotenv import load_dotenv
from llama_index.core import Settings

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.language_models import LLM
from langchain_core.prompts import ChatPromptTemplate

# Importe os componentes
from google_auth import GoogleAuthManager
from calendar_tool import GoogleCalendarTool
from sheets_tool import GoogleSheetsTool
from gmail_tool import GoogleGmailTool
from llama_index.embeddings.gemini import GeminiEmbedding
from rag_setup import get_rag_query_engine
from rag_tool import RAGTool

# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# --- Configura√ß√£o Unificada das Ferramentas e Agentes ---
GOOGLE_TOKEN_PATH = "google_token.json"

# Instru√ß√µes atualizadas para o agente, agora um assistente de suporte
# O prompt agora √© gerenciado pelo LangChain
SYSTEM_PROMPT = """Voc√™ √© um assistente de suporte e produtividade. Sua principal fun√ß√£o √© ajudar os usu√°rios fornecendo informa√ß√µes precisas de documentos internos e executando tarefas no Google Workspace.

Regras importantes:
- Sempre responda em Portugu√™s.
- Para perguntas sobre pol√≠ticas internas, procedimentos, documenta√ß√£o de projetos ou informa√ß√µes da empresa, voc√™ DEVE usar a ferramenta `search_internal_knowledge_base`.
- Voc√™ pode e deve combinar ferramentas. Por exemplo: use `search_internal_knowledge_base` para encontrar uma informa√ß√£o e depois use a ferramenta de `send_email`.
- Para qualquer pergunta sobre agenda, compromissos ou eventos, use as ferramentas do Google Calendar.
- Para qualquer pergunta sobre planilhas, dados ou tabelas, use as ferramentas do Google Sheets.
- Para qualquer pergunta sobre e-mails, como ler, procurar ou enviar, use as ferramentas do Gmail.
- Ao usar a ferramenta `search_emails`, construa a 'query' usando o formato de busca do Gmail (ex: 'subject:palavra-chave', 'from:email@exemplo.com', 'is:unread').
- N√£o invente informa√ß√µes e n√£o diga que voc√™ n√£o tem acesso. Use as ferramentas fornecidas para obter a resposta."""

@st.cache_resource
def get_agents():
    """Cria e armazena em cache as inst√¢ncias dos agentes e ferramentas."""
    # 1. Define todos os escopos necess√°rios para as ferramentas
    all_scopes = list(set(GoogleCalendarTool.SCOPES + GoogleSheetsTool.SCOPES + GoogleGmailTool.SCOPES))

    # 2. Cria o gerenciador de autentica√ß√£o e as ferramentas base do Google
    auth_manager = GoogleAuthManager(scopes=all_scopes, token_path=GOOGLE_TOKEN_PATH)
    calendar = GoogleCalendarTool(auth_manager)
    sheets = GoogleSheetsTool(auth_manager)
    gmail = GoogleGmailTool(auth_manager)
    
    google_tools = calendar.get_tools() + sheets.get_tools() + gmail.get_tools()

    # 3. Define os modelos de LLM
    gemini_llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0, google_api_key=os.getenv("GEMINI_API_KEY"))
    # IMPORTANTE: Verifique o nome exato do seu modelo local executando `ollama list` no terminal
    # e substitua o valor de `model` abaixo pelo nome correto.
    local_llm = ChatOpenAI(model="gpt-oss:20b", base_url="http://localhost:11434/v1", api_key="ollama", temperature=0)

    # 4. Cria o prompt do agente
    prompt_template = ChatPromptTemplate.from_messages(
        [
            ("system", SYSTEM_PROMPT),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ]
    )

    # 5. Fun√ß√£o para criar um agente
    def create_agent(llm: LLM):
        # A configura√ß√£o global do LlamaIndex √© feita dentro de get_rag_query_engine
        # para garantir que seja executada antes da cria√ß√£o do motor de busca.
        rag_tool = RAGTool(
            query_engine=get_rag_query_engine(llm=llm, embed_model=GeminiEmbedding(model_name="models/text-embedding-004", api_key=os.getenv("GEMINI_API_KEY")))
        )
        all_tools = google_tools + [rag_tool]
        agent = create_tool_calling_agent(llm, all_tools, prompt_template)
        return AgentExecutor(agent=agent, tools=all_tools, verbose=True)

    # 6. Cria os agentes
    agents = {
        "Gemini 1.5 Flash": create_agent(gemini_llm),
        "Modelo Local (Ollama)": create_agent(local_llm),
    }
    return agents

# --- Interface Streamlit ---

st.set_page_config(page_title="Assistente Google Workspace", layout="wide")
st.title("ü§ñ Assistente Pessoal para Google Workspace")
st.write("Fa√ßa perguntas em linguagem natural para gerenciar seu Calend√°rio, Planilhas e Gmail.")

agents = get_agents()

agent_choice = st.selectbox("Escolha o modelo de IA:", options=list(agents.keys()))

# --- Gerenciamento do Hist√≥rico de Conversa ---
if "messages" not in st.session_state:
    st.session_state.messages = {}

# Inicializa o hist√≥rico para o agente selecionado, se n√£o existir
if agent_choice not in st.session_state.messages:
    st.session_state.messages[agent_choice] = []

# Exibe as mensagens do hist√≥rico
for message in st.session_state.messages[agent_choice]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Digite sua pergunta..."):
    # Adiciona e exibe a mensagem do usu√°rio
    st.session_state.messages[agent_choice].append({"role": "user", "content": prompt})
    st.chat_message("user").markdown(prompt)

    # Obt√©m o agente selecionado
    selected_agent = agents[agent_choice]

    # Gera e exibe a resposta do assistente
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            # Constr√≥i o hist√≥rico para o LangChain
            chat_history = []
            for msg in st.session_state.messages[agent_choice][:-1]: # Pega todo o hist√≥rico exceto a √∫ltima pergunta
                if msg["role"] == "user":
                    chat_history.append(("human", msg["content"]))
                elif msg["role"] == "assistant":
                    chat_history.append(("ai", msg["content"]))

            # Usa .stream() para respostas em tempo real e st.write_stream para exibir
            stream = selected_agent.stream({"input": prompt, "chat_history": chat_history})
            # O 'output' vem dentro de um dicion√°rio, ent√£o extra√≠mos o valor de cada chunk
            full_response = st.write_stream(chunk["output"] for chunk in stream if "output" in chunk)
            st.session_state.messages[agent_choice].append({"role": "assistant", "content": full_response})

# --- Funcionalidade de Reautentica√ß√£o ---
st.sidebar.header("Configura√ß√µes Avan√ßadas")
if st.sidebar.button("Limpar Autentica√ß√£o e Reautenticar"):
    
    def delete_token_files():
        """Deleta o arquivo de token unificado para for√ßar uma nova autentica√ß√£o."""
        # Agora s√≥ precisamos deletar um arquivo
        token_paths = [GOOGLE_TOKEN_PATH]
        deleted_files = []
        for token_file in token_paths:
            if os.path.exists(token_file):
                try:
                    os.remove(token_file)
                    deleted_files.append(token_file)
                except OSError as e:
                    st.sidebar.error(f"Erro ao deletar {token_file}: {e}")
        return deleted_files

    deleted = delete_token_files()
    if deleted:
        st.sidebar.success(f"Tokens removidos: {', '.join(deleted)}. A pr√≥xima a√ß√£o ir√° solicitar nova autentica√ß√£o.")
        st.cache_resource.clear() # Limpa o cache para recarregar os agentes e o auth_manager
    else:
        st.sidebar.info("Nenhum arquivo de token encontrado para remover.")
    st.rerun()

if st.sidebar.button("Nova Conversa"):
    if agent_choice in st.session_state.messages:
        st.session_state.messages[agent_choice] = []
    st.rerun()